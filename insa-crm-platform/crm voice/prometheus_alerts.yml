# Prometheus Alert Rules for INSA CRM Multi-Agent System
# Version: 1.0.0
# Created: October 29, 2025

groups:
  # ============================================================================
  # Worker Health Alerts - Critical system availability
  # ============================================================================
  - name: worker_health
    interval: 30s
    rules:
      - alert: WorkerUnhealthy
        expr: insa_worker_health_status == 0
        for: 1m
        labels:
          severity: critical
          component: worker
          team: platform
        annotations:
          summary: "Worker {{ $labels.worker_name }} is UNHEALTHY"
          description: |
            Worker {{ $labels.worker_name }} (type: {{ $labels.worker_type }}) has been unhealthy for more than 1 minute.

            Current status: UNHEALTHY
            Worker type: {{ $labels.worker_type }}

            Impact: This worker is not processing requests, which may cause delays or failures.

            Action Required:
            1. Check worker logs: journalctl -u {{ $labels.worker_name }} -n 50
            2. Verify worker process is running: ps aux | grep {{ $labels.worker_name }}
            3. Check resource usage: top -p $(pgrep {{ $labels.worker_name }})
            4. Restart if needed: systemctl restart {{ $labels.worker_name }}
          runbook: "https://docs.insa-crm.com/runbooks/worker-unhealthy"

      - alert: MultipleWorkersDown
        expr: count(insa_worker_health_status == 0) >= 2
        for: 2m
        labels:
          severity: critical
          component: worker
          team: platform
        annotations:
          summary: "Multiple workers are DOWN ({{ $value }} workers)"
          description: |
            {{ $value }} workers are currently unhealthy, indicating a systemic issue.

            This may indicate:
            - Infrastructure problem (CPU/memory/disk)
            - Shared dependency failure (database, cache, message bus)
            - Network connectivity issues

            Immediate Action Required:
            1. Check system resources: free -h && df -h
            2. Check database connectivity
            3. Check message bus status
            4. Review system logs: journalctl -p err -n 100
          runbook: "https://docs.insa-crm.com/runbooks/multiple-workers-down"

      - alert: WorkerHighQueueDepth
        expr: insa_worker_queue_size > 50
        for: 5m
        labels:
          severity: warning
          component: worker
          team: platform
        annotations:
          summary: "Worker {{ $labels.worker_name }} has high queue depth ({{ $value }})"
          description: |
            Worker {{ $labels.worker_name }} queue has {{ $value }} pending tasks (threshold: 50).

            This indicates the worker is overwhelmed or processing too slowly.

            Potential causes:
            - High traffic volume
            - Slow downstream dependencies
            - Resource constraints (CPU/memory)

            Actions:
            1. Monitor queue trend - is it growing?
            2. Check worker processing time
            3. Consider scaling up worker capacity
            4. Review recent traffic patterns
          runbook: "https://docs.insa-crm.com/runbooks/high-queue-depth"

      - alert: WorkerFrequentRestarts
        expr: rate(insa_worker_restarts_total[15m]) > 0.1
        for: 5m
        labels:
          severity: warning
          component: worker
          team: platform
        annotations:
          summary: "Worker {{ $labels.worker_name }} is restarting frequently"
          description: |
            Worker {{ $labels.worker_name }} has restarted more than once in the last 15 minutes.

            Restart rate: {{ $value }} restarts/second

            This indicates instability. Common causes:
            - Memory leaks (OOM kills)
            - Unhandled exceptions
            - Dependency failures causing crashes
            - Configuration issues

            Actions:
            1. Check restart reasons: journalctl -u {{ $labels.worker_name }} | grep -i restart
            2. Review worker logs for crashes
            3. Monitor memory usage trends
            4. Check for recent code deployments
          runbook: "https://docs.insa-crm.com/runbooks/frequent-restarts"

  # ============================================================================
  # Request & Performance Alerts - Service quality
  # ============================================================================
  - name: request_performance
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: |
          (sum(rate(insa_agent_requests_total{status="error"}[5m])) /
           sum(rate(insa_agent_requests_total[5m]))) > 0.05
        for: 5m
        labels:
          severity: critical
          component: api
          team: platform
        annotations:
          summary: "High error rate detected ({{ $value | humanizePercentage }})"
          description: |
            Error rate is {{ $value | humanizePercentage }} (threshold: 5%).

            This indicates a significant number of requests are failing.

            Impact: Users are experiencing errors, degraded service quality.

            Actions:
            1. Check error dashboard for error types
            2. Review recent deployments or changes
            3. Check dependency health (database, external APIs)
            4. Review error logs for patterns
          runbook: "https://docs.insa-crm.com/runbooks/high-error-rate"

      - alert: HighTimeoutRate
        expr: |
          (sum(rate(insa_agent_requests_total{status="timeout"}[5m])) /
           sum(rate(insa_agent_requests_total[5m]))) > 0.02
        for: 5m
        labels:
          severity: warning
          component: api
          team: platform
        annotations:
          summary: "High timeout rate ({{ $value | humanizePercentage }})"
          description: |
            Timeout rate is {{ $value | humanizePercentage }} (threshold: 2%).

            Requests are taking too long and timing out.

            Possible causes:
            - Slow database queries
            - External API latency
            - Insufficient worker capacity
            - Resource exhaustion

            Actions:
            1. Check latency dashboard (P95, P99)
            2. Identify slow operations
            3. Review database query performance
            4. Check external API response times
          runbook: "https://docs.insa-crm.com/runbooks/high-timeout-rate"

      - alert: HighLatencyP95
        expr: |
          histogram_quantile(0.95,
            sum(rate(insa_agent_request_duration_seconds_bucket[5m])) by (le)
          ) > 10
        for: 10m
        labels:
          severity: warning
          component: api
          team: platform
        annotations:
          summary: "High P95 latency detected ({{ $value }}s)"
          description: |
            95th percentile request latency is {{ $value }}s (threshold: 10s).

            This means 5% of requests are taking longer than {{ $value }}s.

            User experience is degraded for some users.

            Actions:
            1. Check latency dashboard for specific agents
            2. Identify slowest operations
            3. Review recent code changes
            4. Check for resource bottlenecks
          runbook: "https://docs.insa-crm.com/runbooks/high-latency"

      - alert: HighLatencyP99
        expr: |
          histogram_quantile(0.99,
            sum(rate(insa_agent_request_duration_seconds_bucket[5m])) by (le)
          ) > 30
        for: 10m
        labels:
          severity: critical
          component: api
          team: platform
        annotations:
          summary: "Very high P99 latency ({{ $value }}s)"
          description: |
            99th percentile latency is {{ $value }}s (threshold: 30s).

            This indicates severe performance issues for outliers.

            Actions:
            1. Investigate slowest operations immediately
            2. Check for resource exhaustion
            3. Review timeout configurations
            4. Consider emergency scaling
          runbook: "https://docs.insa-crm.com/runbooks/very-high-latency"

  # ============================================================================
  # Cache Alerts - Performance optimization
  # ============================================================================
  - name: cache_performance
    interval: 1m
    rules:
      - alert: LowCacheHitRate
        expr: |
          (sum(rate(insa_cache_hits_total[10m])) by (cache_type) /
           (sum(rate(insa_cache_hits_total[10m])) by (cache_type) +
            sum(rate(insa_cache_misses_total[10m])) by (cache_type))) < 0.7
        for: 15m
        labels:
          severity: warning
          component: cache
          team: platform
        annotations:
          summary: "Low cache hit rate for {{ $labels.cache_type }} ({{ $value | humanizePercentage }})"
          description: |
            Cache hit rate for {{ $labels.cache_type }} is {{ $value | humanizePercentage }} (threshold: 70%).

            Low cache efficiency means more expensive operations (database queries, API calls).

            Potential causes:
            - Cache size too small (frequent evictions)
            - Cache TTL too short
            - High cache key diversity (low reuse)
            - Cache warming not effective

            Actions:
            1. Check cache size and eviction rate
            2. Review cache TTL settings
            3. Analyze cache key patterns
            4. Consider increasing cache size
          runbook: "https://docs.insa-crm.com/runbooks/low-cache-hit-rate"

      - alert: HighCacheEvictionRate
        expr: rate(insa_cache_evictions_total{reason="size_limit"}[5m]) > 10
        for: 10m
        labels:
          severity: warning
          component: cache
          team: platform
        annotations:
          summary: "High cache eviction rate for {{ $labels.cache_type }}"
          description: |
            Cache {{ $labels.cache_type }} is evicting {{ $value }} entries/second due to size limit.

            This indicates the cache is too small for the working set.

            Impact: Reduced cache effectiveness, increased load on backend.

            Actions:
            1. Review cache size configuration
            2. Analyze cache entry sizes
            3. Consider increasing cache size
            4. Review eviction policy (LRU vs TTL)
          runbook: "https://docs.insa-crm.com/runbooks/high-eviction-rate"

  # ============================================================================
  # Circuit Breaker Alerts - Failure isolation
  # ============================================================================
  - name: circuit_breaker
    interval: 30s
    rules:
      - alert: CircuitBreakerOpen
        expr: insa_circuit_breaker_state == 1
        for: 2m
        labels:
          severity: critical
          component: circuit_breaker
          team: platform
        annotations:
          summary: "Circuit breaker {{ $labels.breaker_name }} is OPEN"
          description: |
            Circuit breaker {{ $labels.breaker_name }} has been OPEN for more than 2 minutes.

            State: OPEN (rejecting all requests)

            This means the protected service is experiencing failures and the circuit breaker
            has opened to prevent cascading failures.

            Impact: All requests to {{ $labels.breaker_name }} are being rejected immediately.

            Actions:
            1. Check the health of the protected service
            2. Review recent errors in {{ $labels.breaker_name }}
            3. Determine root cause of failures
            4. Fix underlying issue
            5. Circuit will auto-reset to HALF_OPEN after timeout
          runbook: "https://docs.insa-crm.com/runbooks/circuit-breaker-open"

      - alert: CircuitBreakerFlapping
        expr: rate(insa_circuit_breaker_transitions_total[10m]) > 0.5
        for: 5m
        labels:
          severity: warning
          component: circuit_breaker
          team: platform
        annotations:
          summary: "Circuit breaker {{ $labels.breaker_name }} is flapping"
          description: |
            Circuit breaker {{ $labels.breaker_name }} is transitioning states frequently ({{ $value }}/sec).

            This indicates the protected service is unstable:
            - Opening and closing repeatedly
            - Intermittent failures
            - Not fully recovered

            Actions:
            1. Check service health trends
            2. Review circuit breaker thresholds
            3. May need to increase timeout or success threshold
            4. Investigate root cause of intermittent failures
          runbook: "https://docs.insa-crm.com/runbooks/circuit-breaker-flapping"

  # ============================================================================
  # Dead Letter Queue Alerts - Message reliability
  # ============================================================================
  - name: dead_letter_queue
    interval: 1m
    rules:
      - alert: DeadLetterQueueGrowing
        expr: insa_dead_letter_queue_size > 50
        for: 10m
        labels:
          severity: warning
          component: dlq
          team: platform
        annotations:
          summary: "DLQ for {{ $labels.agent_type }} is growing ({{ $value }} messages)"
          description: |
            Dead Letter Queue for {{ $labels.agent_type }} has {{ $value }} messages (threshold: 50).

            This indicates persistent failures that could not be retried successfully.

            Messages in DLQ require manual intervention.

            Actions:
            1. Review DLQ messages: Check error types and patterns
            2. Identify common failure reasons
            3. Fix root cause if possible
            4. Replay failed messages after fix
            5. Archive old messages if resolved
          runbook: "https://docs.insa-crm.com/runbooks/dlq-growing"

      - alert: DeadLetterQueueCritical
        expr: insa_dead_letter_queue_size > 200
        for: 5m
        labels:
          severity: critical
          component: dlq
          team: platform
        annotations:
          summary: "DLQ for {{ $labels.agent_type }} is critically large ({{ $value }} messages)"
          description: |
            Dead Letter Queue for {{ $labels.agent_type }} has {{ $value }} messages (critical threshold: 200).

            This is a critical backlog of failed messages.

            Impact: Potential data loss if not addressed, system degradation.

            Immediate Actions:
            1. Stop incoming traffic if necessary
            2. Investigate failure pattern urgently
            3. Fix root cause
            4. Plan bulk replay strategy
            5. Monitor system resources during replay
          runbook: "https://docs.insa-crm.com/runbooks/dlq-critical"

      - alert: HighDLQReplayFailureRate
        expr: |
          (sum(rate(insa_dead_letter_replay_total{success="false"}[10m])) by (agent_type) /
           sum(rate(insa_dead_letter_replay_total[10m])) by (agent_type)) > 0.5
        for: 15m
        labels:
          severity: warning
          component: dlq
          team: platform
        annotations:
          summary: "High DLQ replay failure rate for {{ $labels.agent_type }} ({{ $value | humanizePercentage }})"
          description: |
            More than 50% of DLQ replay attempts are failing for {{ $labels.agent_type }}.

            This indicates the underlying issue has not been resolved.

            Actions:
            1. Stop replay attempts temporarily
            2. Investigate why replays are still failing
            3. Ensure root cause is truly fixed
            4. Review message format/validity
            5. Resume replay after confirming fix
          runbook: "https://docs.insa-crm.com/runbooks/dlq-replay-failures"

  # ============================================================================
  # Retry Alerts - Transient failure handling
  # ============================================================================
  - name: retry_behavior
    interval: 1m
    rules:
      - alert: LowRetrySuccessRate
        expr: |
          (sum(rate(insa_retry_success_total[15m])) by (agent_type) /
           sum(rate(insa_retry_attempts_total[15m])) by (agent_type)) < 0.5
        for: 20m
        labels:
          severity: warning
          component: retry
          team: platform
        annotations:
          summary: "Low retry success rate for {{ $labels.agent_type }} ({{ $value | humanizePercentage }})"
          description: |
            Retry success rate for {{ $labels.agent_type }} is {{ $value | humanizePercentage }} (threshold: 50%).

            This suggests failures are not transient - they persist across retries.

            Potential issues:
            - Permanent errors being retried (should fail fast)
            - Insufficient retry delay (service not recovering)
            - Underlying service consistently unavailable

            Actions:
            1. Review error types - are they retriable?
            2. Check retry configuration (delays, max attempts)
            3. Investigate underlying service health
            4. Consider implementing backpressure
          runbook: "https://docs.insa-crm.com/runbooks/low-retry-success"

      - alert: ExcessiveRetryAttempts
        expr: rate(insa_retry_attempts_total{attempt_number="3"}[10m]) > 5
        for: 10m
        labels:
          severity: warning
          component: retry
          team: platform
        annotations:
          summary: "Excessive retry attempts for {{ $labels.agent_type }}"
          description: |
            {{ $labels.agent_type }} is making {{ $value }} retry attempts/second on attempt #3.

            High retry rates indicate:
            - Frequent transient failures
            - Unstable downstream dependencies
            - Potentially wasted resources

            Actions:
            1. Check if failures are truly transient
            2. Review downstream service health
            3. Consider implementing circuit breaker
            4. May need to adjust retry strategy
          runbook: "https://docs.insa-crm.com/runbooks/excessive-retries"

  # ============================================================================
  # Database Alerts - Data layer health
  # ============================================================================
  - name: database
    interval: 1m
    rules:
      - alert: HighDatabaseErrorRate
        expr: rate(insa_database_errors_total[5m]) > 1
        for: 5m
        labels:
          severity: critical
          component: database
          team: platform
        annotations:
          summary: "High database error rate ({{ $value }}/sec)"
          description: |
            Database is experiencing {{ $value }} errors per second.

            Common causes:
            - Connection pool exhaustion
            - Query timeouts
            - Database server issues
            - Network connectivity problems

            Impact: Data operations failing, potential data inconsistency.

            Immediate Actions:
            1. Check database server health
            2. Review database logs
            3. Check connection pool metrics
            4. Verify network connectivity
            5. Consider read replica failover if applicable
          runbook: "https://docs.insa-crm.com/runbooks/database-errors"

      - alert: SlowDatabaseQueries
        expr: |
          histogram_quantile(0.95,
            sum(rate(insa_database_operation_duration_seconds_bucket[5m])) by (le, operation)
          ) > 5
        for: 10m
        labels:
          severity: warning
          component: database
          team: platform
        annotations:
          summary: "Slow database queries detected (P95: {{ $value }}s)"
          description: |
            95th percentile database query time is {{ $value }}s for {{ $labels.operation }}.

            Slow queries impact overall system performance.

            Actions:
            1. Identify specific slow queries
            2. Check for missing indexes
            3. Review query execution plans
            4. Check database server resources (CPU, I/O)
            5. Consider query optimization
          runbook: "https://docs.insa-crm.com/runbooks/slow-queries"

  # ============================================================================
  # System Resource Alerts - Infrastructure health
  # ============================================================================
  - name: system_resources
    interval: 1m
    rules:
      - alert: HighMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.9
        for: 5m
        labels:
          severity: warning
          component: system
          team: infrastructure
        annotations:
          summary: "High memory usage on {{ $labels.instance }} ({{ $value | humanizePercentage }})"
          description: |
            Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}.

            Risk of OOM kills, process crashes.

            Actions:
            1. Identify memory-heavy processes: top -o %MEM
            2. Check for memory leaks
            3. Review recent deployments
            4. Consider scaling or adding memory
          runbook: "https://docs.insa-crm.com/runbooks/high-memory"

      - alert: HighCPUUsage
        expr: |
          (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance)) > 0.8
        for: 10m
        labels:
          severity: warning
          component: system
          team: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }} ({{ $value | humanizePercentage }})"
          description: |
            CPU usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}.

            May impact request processing times.

            Actions:
            1. Identify CPU-intensive processes
            2. Check for runaway processes
            3. Review recent code changes
            4. Consider scaling or optimization
          runbook: "https://docs.insa-crm.com/runbooks/high-cpu"

      - alert: DiskSpaceLow
        expr: |
          (node_filesystem_avail_bytes{fstype!="tmpfs"} /
           node_filesystem_size_bytes{fstype!="tmpfs"}) < 0.1
        for: 5m
        labels:
          severity: critical
          component: system
          team: infrastructure
        annotations:
          summary: "Low disk space on {{ $labels.instance }}:{{ $labels.mountpoint }} ({{ $value | humanizePercentage }} free)"
          description: |
            Only {{ $value | humanizePercentage }} disk space remaining on {{ $labels.mountpoint }}.

            Risk: System failures, log rotation issues, database corruption.

            Immediate Actions:
            1. Identify large files: du -sh /* | sort -rh | head -10
            2. Clean up old logs
            3. Remove temporary files
            4. Archive or delete old data
            5. Add more disk space if needed
          runbook: "https://docs.insa-crm.com/runbooks/disk-space-low"
